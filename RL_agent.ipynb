{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chess\n",
    "import re \n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "from stable_baselines3 import A2C\n",
    "from stable_baselines3.common.policies import ActorCriticPolicy\n",
    "from torch.nn import ReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {},
   "outputs": [],
   "source": [
    "MOVE_LIST_REGEX = r'\\((.*?)\\)'\n",
    "MAX_MOVES_IN_ANY_POS = 218\n",
    "\n",
    "board = chess.Board()\n",
    "starting_pos = np.zeros((8, 8, 12), dtype=np.uint8)\n",
    "for square, piece in board.piece_map().items():\n",
    "    piece_type = piece.piece_type\n",
    "    piece_color = int(piece.color)\n",
    "    starting_pos[square // 8][square % 8][piece_type - 1 + 6 * piece_color] = 1\n",
    "\n",
    "STARTING_POS_TENSOR = starting_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_king_square(board: chess.Board, white: bool):\n",
    "    for square, piece in board.piece_map().items():\n",
    "        if white:\n",
    "            if piece == chess.KING and piece.color == chess.WHITE:\n",
    "                return square\n",
    "        else:\n",
    "            if piece == chess.KING and piece.color == chess.BLACK:\n",
    "                return square\n",
    "            \n",
    "KNIGHT_DANGER = 0.8\n",
    "BISHOP_DANGER = 0.7\n",
    "ROOK_DANGER = 0.5\n",
    "QUEEN_DANGER = 0.2\n",
    "def get_king_tropism(board: chess.Board):\n",
    "    weighted_danger = 0\n",
    "    num_enemy_pieces = 0\n",
    "    relevant_pieces_dict = {}\n",
    "    if board.turn == chess.BLACK: # ie White just made a move\n",
    "        # Locate white king and black pieces\n",
    "        w_king_square = get_king_square(board, True)\n",
    "        for square, piece in board.piece_map().items():\n",
    "            if piece.color == chess.BLACK:\n",
    "                match piece:\n",
    "                    case chess.KNIGHT: # We put the value in a list because there could be multiple knights\n",
    "                        relevant_pieces_dict[chess.KNIGHT] += [\n",
    "                            KNIGHT_DANGER * chess.square_knight_distance(square, w_king_square)]\n",
    "                    case chess.BISHOP:\n",
    "                        relevant_pieces_dict[chess.BISHOP] += [\n",
    "                            BISHOP_DANGER * chess.square_distance(square, w_king_square)]\n",
    "                    case chess.ROOK:\n",
    "                        relevant_pieces_dict[chess.ROOK] += [\n",
    "                            ROOK_DANGER * chess.square_distance(square, w_king_square)]\n",
    "                    case chess.QUEEN:\n",
    "                        relevant_pieces_dict[chess.QUEEN] += [\n",
    "                            QUEEN_DANGER * chess.square_distance(square, w_king_square)]\n",
    "\n",
    "    else:\n",
    "        b_king_square = get_king_square(board, False)\n",
    "        for square, piece in board.piece_map().items():\n",
    "            if piece.color == chess.WHITE:\n",
    "                match piece:\n",
    "                    case chess.KNIGHT: # We put the value in a list because there could be multiple pieces of this type\n",
    "                        relevant_pieces_dict[chess.KNIGHT] += [\n",
    "                            KNIGHT_DANGER * chess.square_knight_distance(square, b_king_square)]\n",
    "                    case chess.BISHOP:\n",
    "                        relevant_pieces_dict[chess.BISHOP] += [\n",
    "                            BISHOP_DANGER * chess.square_distance(square, b_king_square)]\n",
    "                    case chess.ROOK:\n",
    "                        relevant_pieces_dict[chess.ROOK] += [\n",
    "                            ROOK_DANGER * chess.square_distance(square, b_king_square)]\n",
    "                    case chess.QUEEN:\n",
    "                        relevant_pieces_dict[chess.QUEEN] += [\n",
    "                            QUEEN_DANGER * chess.square_distance(square, b_king_square)]\n",
    "    \n",
    "    for _, dist_list in relevant_pieces_dict.items():\n",
    "        for weighted_distance in dist_list:\n",
    "            weighted_danger += weighted_distance\n",
    "            num_enemy_pieces += 1\n",
    "    \n",
    "    return weighted_danger / num_enemy_pieces if num_enemy_pieces > 0 else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_move_list(board: chess.Board):\n",
    "    moves = [move.strip() for move in re.findall(MOVE_LIST_REGEX, str(board.legal_moves))[0].split(',')]\n",
    "    move_dict = {i: move for i, move in enumerate(moves)}\n",
    "    return move_dict\n",
    "\n",
    "class ChessEnvironment(gym.Env):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Define the observation and action spaces\n",
    "        # We represent the board with an 8x8 grid, then imagine one-hot encoding tensors extending\n",
    "        # into the 3rd dimension of the chessboard, representing what piece is where (both side and color)\n",
    "        # Since there are 6 pieces per side (pawn, knight, bishop, rook, queen, and king), there are 12 total\n",
    "        self.observation_space = gym.spaces.Box(low=0, high=1, shape=(8, 8, 12), dtype=np.uint8)\n",
    "        # Initialize the chessboards\n",
    "        self.board = chess.Board()\n",
    "        self.last_board = self.board\n",
    "        # Action space \n",
    "        self.move_dict = make_move_list(self.board)\n",
    "        self.action_space = gym.spaces.MultiDiscrete([8, 8, 73])\n",
    "\n",
    "        self.piece_value_dict = {\n",
    "            chess.PAWN: 1,\n",
    "            chess.KNIGHT: 3,\n",
    "            chess.BISHOP: 3.25, # Maybe change this\n",
    "            chess.ROOK: 5,\n",
    "            chess.QUEEN: 9,\n",
    "        }\n",
    "        # Hyperparameters (for reward function)\n",
    "        self.flex_weight = 0.25\n",
    "        self.material_weight = 0.5\n",
    "        self.king_safety_weight = 0.25\n",
    "        self.move_penalty_base = 0.01\n",
    "        self.draw_base = 0.01\n",
    "        self.move_list = []\n",
    "\n",
    "        # Policy kwargs for A2C model\n",
    "        self.policy_kwargs = dict(\n",
    "            net_arch=dict(pi=[128, 128, 64], vf=[128, 128, 64]),\n",
    "            activation_fn=ReLU,\n",
    "            #use_sde=True, # Some errors getting 'two values provided' error, look into this later\n",
    "        )\n",
    "    \n",
    "    def flexibility_reward(self): # Reward computer for making moves that limit opponents options\n",
    "        legal_move_count = -len(list(self.board.legal_moves))\n",
    "        # Normalizing\n",
    "        legal_move_count = legal_move_count / MAX_MOVES_IN_ANY_POS\n",
    "\n",
    "        return legal_move_count * self.flex_weight\n",
    "    \n",
    "    def get_net_material(self, board: chess.Board):\n",
    "        difference = 0\n",
    "        for _, piece in board.piece_map().items():\n",
    "            value = self.piece_value_dict.get(piece.piece_type, 0) # default to 0 if no piece on square\n",
    "            if piece.color == chess.WHITE:\n",
    "                difference += value\n",
    "            else:\n",
    "                difference -= value\n",
    "        return difference\n",
    "\n",
    "    def material_reward(self):\n",
    "        difference = self.get_net_material(self.board) \n",
    "        # bring extreme outliers in line with rest of data (since once you're that far ahead it doesn't really matter)\n",
    "        # Also normalizing the value\n",
    "        difference = np.clip(difference, -15.0, 15.0) / 15\n",
    "        return difference * self.material_weight\n",
    "    \n",
    "    def king_safety_reward(self):\n",
    "        # We're going to use something called \"King Tropism\", where we calculate the distance from enemy pieces to the king\n",
    "        # Note: this technique I'm just fully yoinking from chessprogramming.org/King_Safety\n",
    "        # Go check them out if you're trying something similar to this, they are very helpful\n",
    "        val = get_king_tropism(self.board)\n",
    "        if val > 0:\n",
    "            val = 1 / val # get reciprocal since small values mean pieces are close ie danger is high, large values mean pieces are far away ie danger low\n",
    "        return val * self.king_safety_weight\n",
    "        \n",
    "    \n",
    "    def calculate_reward(self):\n",
    "        # Technically board.turn is True when it is White's turn, and false when Black's, but when this fn is called, \n",
    "        # the move has already been made on the board, and thus the turn player has swapped. This will allow us to\n",
    "        # evaluate the previous move from the perspective of that side\n",
    "        evaluatingBlack = bool(self.board.turn)\n",
    "        game_status = self.board.result()\n",
    "        if (evaluatingBlack and game_status == '0-1') or (not evaluatingBlack and game_status == '1-0'): # Agent delivered checkmate on the previous move\n",
    "            # We're gonna normalize the position evaluation heuristics so this is 8x the max value for that\n",
    "            return 8.0 \n",
    "        elif game_status == '1/2-1/2': #Draw\n",
    "            # Check the ply before the draw for the material imbalance--we want to reward draws found while down material\n",
    "            # while punishing draws while up material\n",
    "            # Will be positive if white has more material, else negative\n",
    "            mat_difference = self.get_net_material(self.last_board)\n",
    "            #Drew with more material, negative reward scaled w how much material\n",
    "            if (evaluatingBlack and mat_difference < 0) or (not evaluatingBlack and mat_difference > 0):\n",
    "                return -1 * abs(mat_difference) * self.draw_base\n",
    "            # Drew with less material, positive reward scaled with the material difference\n",
    "            elif (evaluatingBlack and mat_difference > 0) or (not evaluatingBlack and mat_difference < 0):\n",
    "                return abs(mat_difference) * self.draw_base\n",
    "        # In case where no checkmate has been achieved, use heuristics\n",
    "        flex_score = self.flexibility_reward()\n",
    "        material_score = self.material_reward()\n",
    "        if evaluatingBlack: # Invert value if eval'ing Black so we're not feeding negative into reward val\n",
    "            material_score *= -1\n",
    "        king_safety_score = self.king_safety_reward()\n",
    "        move_penalty = self.board.ply() * self.move_penalty_base\n",
    "        return (flex_score + material_score - king_safety_score) / 3 - move_penalty\n",
    "        \n",
    "            \n",
    "\n",
    "    def sync_action_space(self):\n",
    "        self.move_dict = make_move_list(self.board)\n",
    "        self.action_space = gym.spaces.Discrete(len(self.move_dict))\n",
    "    \n",
    "    def reset(self, seed=None, options=None):\n",
    "        # Reset the chessboard to the starting position\n",
    "        self.board = chess.Board()\n",
    "        self.sync_action_space()\n",
    "        self.move_list = []\n",
    "        # Return the initial observation\n",
    "        return STARTING_POS_TENSOR, {}\n",
    "\n",
    "    def step(self, action):\n",
    "        move = self.move_dict[action]\n",
    "        self.move_list.append(move)\n",
    "        # Save the board as is before making the move for reward calculation\n",
    "        self.last_board = self.board\n",
    "        # Execute the specified action on the chessboard\n",
    "        self.board.push_san(move)\n",
    "        self.sync_action_space()\n",
    "\n",
    "        # Convert the board to the observation format\n",
    "        observation = np.zeros((8, 8, 12), dtype=np.uint8)\n",
    "        for square, piece in self.board.piece_map().items():\n",
    "            piece_type = piece.piece_type\n",
    "            piece_color = int(piece.color)\n",
    "            observation[square // 8][square % 8][piece_type - 1 + 6 * piece_color] = 1\n",
    "\n",
    "\n",
    "        # Calculate the reward\n",
    "        reward = self.calculate_reward()\n",
    "        # Check if the episode is done\n",
    "        terminated = (self.board.result() != '*')\n",
    "        # Return the observation, reward, done flag, and additional info\n",
    "        return observation, reward, terminated, False, {}\n",
    "\n",
    "    def render(self):\n",
    "        out_str = \"\"\n",
    "        move_num = 1\n",
    "        for ind, move in enumerate(self.move_list):\n",
    "            if ind % 2 == 0:\n",
    "                out_str += f\"{move_num}.{move} \"\n",
    "                move_num += 1\n",
    "            else:\n",
    "                out_str += f\"{move} \"\n",
    "        \n",
    "        print(out_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    }
   ],
   "source": [
    "env = ChessEnvironment()\n",
    "model = A2C(ActorCriticPolicy, env, policy_kwargs=env.policy_kwargs, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.f3 h5 2.g3 g6 3.b3 Bh6 4.Nh3 f6 5.Na3 c6 6.Nb5 Bg5 7.Nc7+ Kf7 8.Nxa8 Qa5 9.Nb6 Nh6 10.Na4 Ke8 11.Nc3 Qc7 12.Bg2 Rf8 13.Na4 Qd8 14.Rg1 Kf7 15.f4 Ng8 16.Nb2 b6 17.Bf1 Bh6 18.e3 Bb7 19.Nf2 Ba8 20.Qg4 b5 21.Qh3 Qa5 22.Nbd3 Rc8 23.Rg2 Bf8 24.Be2 Qb4 25.Qg4 Ke8 26.Ne5 Bh6 27.Nxg6 Bf8 28.Qxd7+ Kf7 29.Qxc8 Nh6 30.Qxf8+ Kxg6 31.Qh8 Nd7 32.Qg8+ Nxg8 33.Bb2 Nb6 34.Bd4 Nd5 35.Bb6 Qa5 36.Bf1 Qa3 37.Kd1 Qxb3 38.Bd8 Nc7 39.Ke2 Qxe3+ 40.Kxe3 Bb7 41.Nd3 Na6 42.Bb6 Bc8 43.Nf2 Nc7 44.Kd4 Kf5 45.Kc3 b4+ 46.Kb2 Nb5 47.Bd3+ Ke6 48.Bc7 Kf7 49.Be2 Nc3 50.Bb6 Nd1+ 51.Kb3 Kg6 52.Nd3 Be6+ 53.c4 Bd7 54.Nc1 Nc3 55.Bc5 Nb5 56.Bxe7 Bg4 57.Bc5 Nc7 58.Bg1 Nb5 59.Bf1 Kg7 60.Kc2 Nc3 61.Kd3 Kh7 62.h3 Be2+ 63.Kc2 Bg4 64.Re2 Nb5 65.Re8 Kh8 66.Bc5 Bf5+ 67.Bd3 a6 68.Bd6 Be4 69.Rd8 a5 70.Rxg8+ Kxg8 71.Ne2 Nd4+ 72.Nxd4 Bg6 73.Bf5 c5 74.Kb2 Bxf5 75.Rg1 Kh7 76.Nc2 b3 77.Ne1 Bxh3 78.Bf8 Kg8 79.Bh6 bxa2 80.Rh1 Kh7 81.g4 a1=R 82.Bf8 Rc1 83.gxh5 Bf5 84.Rg1 Rxe1 85.Bd6 Re2 86.Ka3 Be6 87.Rg3 Bd7 88.Rf3 Bc6 89.Be7 Re4 90.Re3 Kg8 91.Rxe4 a4 92.Bd6 f5 93.Re5 Ba8 94.Kxa4 Bd5 95.Re6 Be4 96.h6 Bc2+ 97.Ka3 Be4 98.Re8+ Kf7 99.Rc8 Bh1 100.Kb2 Bb7 101.Bc7 Be4 102.Re8 Bb7 103.Rf8+ Kxf8 104.Bb6 Bg2 105.Ka1 Ba8 106.Ka2 Bd5 107.d3 Bf3 108.h7 Bd1 109.Kb2 Bh5 110.h8=N Bf3 111.Ka2 Ke8 112.Nf7 Bc6 113.d4 Ba8 114.Nh6 cxd4 115.Ka1 Kd7 116.Ba7 Kc6 117.Bb6 Kxb6 118.Ka2 Kb7 119.Ka1 Kb6 120.Nxf5 Bg2 121.Ka2 Bh3 122.Ka1 Kc7 123.Nxd4 Bg4 124.Kb1 Kb6 125.Ka2 Bd7 126.Ne2 Be6 127.Kb1 Ka7 128.c5 Bf7 129.Nc3 Kb8 130.Nb5 Be8 131.Nc7 Bb5 132.Kc1 Kc8 133.Kb2 Bd7 134.Na8 Bc6 135.Ka2 Kd8 136.Nc7 Ke7 137.Ne6 Kf6 138.Nc7 Kf5 139.Kb1 Be8 140.Na8 Ke4 141.Kc2 Bb5 142.Kc1 Kd5 143.Kd1 Ke4 144.Kc1 Kxf4 145.Nb6 Kf5 146.Kc2 Kf6 147.Kd2 Bc6 148.Nd5+ Kf5 149.Kc1 Bd7 150.Nc3 Ke6 151.Ne4 Kd5 152.Nd2 Kc6 153.Nb1 Kd5 154.Kc2 Bc8 155.Kb2 Bg4 156.Na3 Bd7 157.Kb3 Bc8 158.Nb1 Kxc5 159.Nd2 Kc6 160.Nf3 Kc5 161.Kc2 Kc4 162.Nd4 Be6 163.Kb1 Bg8 164.Ka1 Bf7 165.Ka2 Bh5 166.Ka1 Kd3 167.Nf3 Bg6 168.Kb2 Bh5 169.Kb1 Be8 170.Ka1 Bf7 171.Nh2 Be6 172.Kb1 Bc4 173.Kc1 Bb5 174.Nf1 Ke4 175.Nh2 Kd4 176.Ng4 Be8 177.Nh2 Bc6 178.Kb2 Kd3 179.Ka2 Ke2 180.Ka1 Bb7 181.Ka2 Kd1 182.Kb1 Bg2 183.Nf1 Be4+ 184.Kb2 Bh7 185.Nd2 Ke1 186.Nb1 Bg8 187.Kc1 Ba2 188.Na3 Be6 189.Nb1 Bg4 190.Na3 Ke2 191.Kb2 Ke1 192.Kc2 Bd1+ 193.Kb2 Be2 194.Ka1 Bd3 195.Nb5 Bc4 196.Na7 Be6 197.Nb5 Kf1 198.Nc3 Kg1 199.Na4 Kh1 200.Nc5 Bf7 201.Kb2 Bg8 202.Nd7 Kg2 203.Nb8 Kh2 204.Kc2 Kh1 205.Kb1 Be6 206.Na6 Bc8 207.Nb8 Bd7 208.Kc2 Bc8 209.Kb3 Bg4 210.Nc6 Be2 211.Ka4 Bd1+ 212.Kb4 Kg2 213.Nd8 Bg4 214.Nf7 Kg1 215.Nh8 Bh5 216.Kb5 Bg4 217.Ka5 Bh3 218.Ng6 Bf5 219.Kb4 Bc2 220.Kc3 Ba4 221.Nf4 Bd7 222.Nh5 Kf1 223.Ng7 Kf2 224.Ne6 Ke2 225.Kb2 Kd1 226.Nf4 Ba4 227.Nd3 Bb5 228.Nc1 Kd2 229.Ka1 Ke3 230.Nd3 Ke4 231.Nf4 Bf1 232.Nh5 Ba6 233.Kb1 Bd3+ \n",
      "Games completed: 0/100000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[425], line 19\u001b[0m\n\u001b[0;32m     16\u001b[0m     available_moves \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(env\u001b[39m.\u001b[39mmove_dict)\n\u001b[0;32m     17\u001b[0m     \u001b[39mwhile\u001b[39;00m action \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m available_moves:\n\u001b[0;32m     18\u001b[0m         \u001b[39m# Make more predictions until one is in range of the indices\u001b[39;00m\n\u001b[1;32m---> 19\u001b[0m         action, _ \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mpredict(observation)\n\u001b[0;32m     20\u001b[0m     observation, reward, done, _, _ \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39mstep(\u001b[39mint\u001b[39m(action))\n\u001b[0;32m     21\u001b[0m \u001b[39mif\u001b[39;00m feature_game:\n",
      "File \u001b[1;32mc:\\Users\\SpiceBoi\\Desktop\\ML Projects\\mlenv\\Lib\\site-packages\\stable_baselines3\\common\\base_class.py:555\u001b[0m, in \u001b[0;36mBaseAlgorithm.predict\u001b[1;34m(self, observation, state, episode_start, deterministic)\u001b[0m\n\u001b[0;32m    535\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpredict\u001b[39m(\n\u001b[0;32m    536\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    537\u001b[0m     observation: Union[np\u001b[39m.\u001b[39mndarray, Dict[\u001b[39mstr\u001b[39m, np\u001b[39m.\u001b[39mndarray]],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    540\u001b[0m     deterministic: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m    541\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[np\u001b[39m.\u001b[39mndarray, Optional[Tuple[np\u001b[39m.\u001b[39mndarray, \u001b[39m.\u001b[39m\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m]]]:\n\u001b[0;32m    542\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    543\u001b[0m \u001b[39m    Get the policy action from an observation (and optional hidden state).\u001b[39;00m\n\u001b[0;32m    544\u001b[0m \u001b[39m    Includes sugar-coating to handle different observations (e.g. normalizing images).\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    553\u001b[0m \u001b[39m        (used in recurrent policies)\u001b[39;00m\n\u001b[0;32m    554\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 555\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpolicy\u001b[39m.\u001b[39;49mpredict(observation, state, episode_start, deterministic)\n",
      "File \u001b[1;32mc:\\Users\\SpiceBoi\\Desktop\\ML Projects\\mlenv\\Lib\\site-packages\\stable_baselines3\\common\\policies.py:349\u001b[0m, in \u001b[0;36mBasePolicy.predict\u001b[1;34m(self, observation, state, episode_start, deterministic)\u001b[0m\n\u001b[0;32m    346\u001b[0m observation, vectorized_env \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobs_to_tensor(observation)\n\u001b[0;32m    348\u001b[0m \u001b[39mwith\u001b[39;00m th\u001b[39m.\u001b[39mno_grad():\n\u001b[1;32m--> 349\u001b[0m     actions \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_predict(observation, deterministic\u001b[39m=\u001b[39;49mdeterministic)\n\u001b[0;32m    350\u001b[0m \u001b[39m# Convert to numpy, and reshape to the original action shape\u001b[39;00m\n\u001b[0;32m    351\u001b[0m actions \u001b[39m=\u001b[39m actions\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mnumpy()\u001b[39m.\u001b[39mreshape((\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39maction_space\u001b[39m.\u001b[39mshape))\n",
      "File \u001b[1;32mc:\\Users\\SpiceBoi\\Desktop\\ML Projects\\mlenv\\Lib\\site-packages\\stable_baselines3\\common\\policies.py:679\u001b[0m, in \u001b[0;36mActorCriticPolicy._predict\u001b[1;34m(self, observation, deterministic)\u001b[0m\n\u001b[0;32m    671\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_predict\u001b[39m(\u001b[39mself\u001b[39m, observation: th\u001b[39m.\u001b[39mTensor, deterministic: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m th\u001b[39m.\u001b[39mTensor:\n\u001b[0;32m    672\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    673\u001b[0m \u001b[39m    Get the action according to the policy for a given observation.\u001b[39;00m\n\u001b[0;32m    674\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    677\u001b[0m \u001b[39m    :return: Taken action according to the policy\u001b[39;00m\n\u001b[0;32m    678\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 679\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_distribution(observation)\u001b[39m.\u001b[39mget_actions(deterministic\u001b[39m=\u001b[39mdeterministic)\n",
      "File \u001b[1;32mc:\\Users\\SpiceBoi\\Desktop\\ML Projects\\mlenv\\Lib\\site-packages\\stable_baselines3\\common\\policies.py:713\u001b[0m, in \u001b[0;36mActorCriticPolicy.get_distribution\u001b[1;34m(self, obs)\u001b[0m\n\u001b[0;32m    706\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    707\u001b[0m \u001b[39mGet the current policy distribution given the observations.\u001b[39;00m\n\u001b[0;32m    708\u001b[0m \n\u001b[0;32m    709\u001b[0m \u001b[39m:param obs:\u001b[39;00m\n\u001b[0;32m    710\u001b[0m \u001b[39m:return: the action distribution.\u001b[39;00m\n\u001b[0;32m    711\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    712\u001b[0m features \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39mextract_features(obs, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpi_features_extractor)\n\u001b[1;32m--> 713\u001b[0m latent_pi \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmlp_extractor\u001b[39m.\u001b[39;49mforward_actor(features)\n\u001b[0;32m    714\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_action_dist_from_latent(latent_pi)\n",
      "File \u001b[1;32mc:\\Users\\SpiceBoi\\Desktop\\ML Projects\\mlenv\\Lib\\site-packages\\stable_baselines3\\common\\torch_layers.py:225\u001b[0m, in \u001b[0;36mMlpExtractor.forward_actor\u001b[1;34m(self, features)\u001b[0m\n\u001b[0;32m    224\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward_actor\u001b[39m(\u001b[39mself\u001b[39m, features: th\u001b[39m.\u001b[39mTensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m th\u001b[39m.\u001b[39mTensor:\n\u001b[1;32m--> 225\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpolicy_net(features)\n",
      "File \u001b[1;32mc:\\Users\\SpiceBoi\\Desktop\\ML Projects\\mlenv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\SpiceBoi\\Desktop\\ML Projects\\mlenv\\Lib\\site-packages\\torch\\nn\\modules\\container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[0;32m    216\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[1;32m--> 217\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[0;32m    218\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\SpiceBoi\\Desktop\\ML Projects\\mlenv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\SpiceBoi\\Desktop\\ML Projects\\mlenv\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Manually train the model\n",
    "total_timesteps = int(1e5)\n",
    "checkpoint_timestep = int(1e4)\n",
    "log_interval = 100\n",
    "\n",
    "for timestep in range(total_timesteps):\n",
    "    observation, _ = env.reset()\n",
    "    done = False\n",
    "    feature_game = timestep % log_interval == 0\n",
    "    save_checkpoint_model = timestep % checkpoint_timestep == 0 and timestep != 0\n",
    "\n",
    "    while not done:\n",
    "        action, _ = model.predict(observation)\n",
    "        # JANKY SOLUTION: IF ISSUES LOOK HERE!!!\n",
    "        # For some reason when one side is in check there's indexing issues so\n",
    "        available_moves = len(env.move_dict)\n",
    "        while action >= available_moves:\n",
    "            # Make more predictions until one is in range of the indices\n",
    "            action, _ = model.predict(observation)\n",
    "        observation, reward, done, _, _ = env.step(int(action))\n",
    "    if feature_game:\n",
    "        env.render()\n",
    "        print(f\"Games completed: {timestep}/{total_timesteps}\")\n",
    "    \n",
    "    if save_checkpoint_model:\n",
    "        model.save(\"a2c_chess_model\" + str(timestep) + \"_games_exp\")\n",
    "\n",
    "# Save the trained model\n",
    "model.save(\"a2c_chess_model_v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = A2C.load('a2c_chess_model_v1')\n",
    "old_model = A2C.load('a2c_chess_model10000_games_exp.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Game 1/10 completed.\n",
      "\n",
      "Game 2/10 completed.\n",
      "\n",
      "Game 3/10 completed.\n",
      "\n",
      "Game 4/10 completed.\n",
      "\n",
      "Game 5/10 completed.\n",
      "\n",
      "Game 6/10 completed.\n",
      "\n",
      "Game 7/10 completed.\n",
      "\n",
      "Game 8/10 completed.\n",
      "\n",
      "Game 9/10 completed.\n",
      "\n",
      "Game 10/10 completed.\n",
      "Results:\n",
      "Model 1 (100k games) wins: 0\n",
      "Model 2 (10k games) wins: 1\n",
      "Draws: 8\n",
      "[223, 191, 192, 234, 274, 205, 234, 48, 231]\n"
     ]
    }
   ],
   "source": [
    "def make_move(move, board: chess.Board):\n",
    "    board.push_san(move)\n",
    "    return board\n",
    "\n",
    "def get_obs_from_board(board: chess.Board):\n",
    "    observation = np.zeros((8, 8, 12), dtype=np.uint8)\n",
    "    for square, piece in board.piece_map().items():\n",
    "        piece_type = piece.piece_type\n",
    "        piece_color = int(piece.color)\n",
    "        observation[square // 8][square % 8][piece_type - 1 + 6 * piece_color] = 1\n",
    "    return observation\n",
    "\n",
    "def play_games(model1, model2, num_games=10):\n",
    "    wins_model1, wins_model2, draws = 0, 0, 0\n",
    "    move_counts = []\n",
    "\n",
    "    for game in range(num_games):\n",
    "        obs = STARTING_POS_TENSOR\n",
    "        env = ChessEnvironment()\n",
    "\n",
    "        done = False\n",
    "        while not done:\n",
    "            if env.board.is_checkmate() or env.board.can_claim_draw():\n",
    "                done = True\n",
    "                draws += 1\n",
    "                move_counts.append(env.board.ply() // 2)\n",
    "                env.render()\n",
    "                break\n",
    "\n",
    "            # Model 1 (100k games model)\n",
    "            env.sync_action_space()\n",
    "            action_idx, _ = model1.predict(obs)\n",
    "            available_moves = env.board.legal_moves.count()\n",
    "            if available_moves == 0:\n",
    "                # If no available moves, end the game\n",
    "                env.render()\n",
    "                done = True\n",
    "                continue\n",
    "            while action_idx >= available_moves:\n",
    "                # Make more predictions until one is in range of the indices\n",
    "                action_idx, _ = model1.predict(obs)\n",
    "\n",
    "            white_move = env.move_dict[int(action_idx)]\n",
    "            #print(\"Model 1 (100k games) move: \", black_move)\n",
    "            make_move(white_move, env.board)\n",
    "            obs = get_obs_from_board(env.board)\n",
    "\n",
    "            if env.board.is_checkmate():\n",
    "                done = True\n",
    "                env.render()\n",
    "                move_counts.append(env.board.ply() // 2)\n",
    "                wins_model2 += 1\n",
    "                break\n",
    "\n",
    "            # Model 2 (10k games model)\n",
    "            env.sync_action_space()\n",
    "            action_idx, _ = model2.predict(obs)\n",
    "            available_moves = env.board.legal_moves.count()\n",
    "            while action_idx >= available_moves:\n",
    "                # Make more predictions until one is in range of the indices\n",
    "                action_idx, _ = model2.predict(obs)\n",
    "\n",
    "            black_move = env.move_dict[int(action_idx)]\n",
    "            #print(\"Model 2 (10k games) move: \", black_move)\n",
    "            make_move(black_move, env.board)\n",
    "            obs = get_obs_from_board(env.board)\n",
    "\n",
    "            if env.board.is_checkmate():\n",
    "                done = True\n",
    "                env.render()\n",
    "                move_counts.append(env.board.ply() // 2)\n",
    "                wins_model1 += 1\n",
    "                break\n",
    "\n",
    "            env.sync_action_space()\n",
    "\n",
    "        print(f\"Game {game + 1}/{num_games} completed.\")\n",
    "\n",
    "    print(\"Results:\")\n",
    "    print(f\"Model 1 (100k games) wins: {wins_model1}\")\n",
    "    print(f\"Model 2 (10k games) wins: {wins_model2}\")\n",
    "    print(f\"Draws: {draws}\")\n",
    "    return move_counts\n",
    "\n",
    "# Load the models\n",
    "model1 = A2C.load('a2c_chess_model_v1')\n",
    "model2 = A2C.load('a2c_chess_model10000_games_exp.zip')\n",
    "\n",
    "# Play the games\n",
    "counts = play_games(model1, model2, num_games=10)\n",
    "print(counts)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
