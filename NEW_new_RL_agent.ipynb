{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tianshou as ts\n",
    "import numpy as np\n",
    "from pettingzoo.classic.chess import chess\n",
    "import torch as T\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import gymnasium as gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "STATE_SHAPE = (8, 8, 111)\n",
    "ACTION_SHAPE = gym.spaces.Discrete(8*8*73)\n",
    "DUMMY_CONV_INPUT = T.zeros(1, *STATE_SHAPE).permute(0, 3, 1, 2)\n",
    "\n",
    "LEARNING_RATE = 1e-3\n",
    "DISCOUNT_FACTOR = 0.9\n",
    "NUM_STEPS_TO_LOOK_AHEAD = 3\n",
    "NUM_STEPS_BETWEEN_TARGET_NETWORK_UPDATES = 320\n",
    "\n",
    "TRAIN_ENV_COUNT = 2\n",
    "TEST_ENV_COUNT = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_env():\n",
    "    return ts.env.PettingZooEnv(chess.env())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepQNetwork(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        # Identify structural patterns using convolutional layers\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=STATE_SHAPE[-1], out_channels=32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        # Calculate the size of the output from convolutional layers\n",
    "        with T.no_grad():\n",
    "            conv_output_size = self.conv_layers(DUMMY_CONV_INPUT).reshape(1, -1).size(1)\n",
    "\n",
    "\n",
    "        # Feed that convolutional output into a fully connected linear net to parse lower-level details\n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Linear(conv_output_size, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, ACTION_SHAPE.n),\n",
    "        )\n",
    "\n",
    "    def forward(self, x, state=None, info={}) -> T.Tensor:\n",
    "        x = x.permute(0, 3, 1, 2)  if isinstance(x, T.Tensor) else T.from_numpy(x).permute(0, 3, 1, 2)\n",
    "        x = x.float()\n",
    "        x = self.conv_layers(x)\n",
    "        x = x.reshape(x.size(0), -1)\n",
    "        q_values = self.fc_layers(x).reshape(-1, ACTION_SHAPE.n)\n",
    "        return q_values, state\n",
    "\n",
    "    def seed(self, seed):\n",
    "        return np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = get_env()\n",
    "q_network = DeepQNetwork()\n",
    "optimizer = optim.Adam(q_network.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "policy = ts.policy.DQNPolicy(\n",
    "    q_network, \n",
    "    optimizer, \n",
    "    discount_factor=DISCOUNT_FACTOR, \n",
    "    estimation_step=NUM_STEPS_TO_LOOK_AHEAD, \n",
    "    target_update_freq=NUM_STEPS_BETWEEN_TARGET_NETWORK_UPDATES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_envs = ts.env.DummyVectorEnv([get_env for _ in range(TRAIN_ENV_COUNT)]) # Check these ranges if badness\n",
    "test_envs = ts.env.DummyVectorEnv([get_env for _ in range(TEST_ENV_COUNT)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_collector = ts.data.Collector(policy, train_envs, ts.data.VectorReplayBuffer(20000, TRAIN_ENV_COUNT), exploration_noise=True)\n",
    "test_collector = ts.data.Collector(policy, test_envs, exploration_noise=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #1:   0%|          | 10/10000 [00:00<05:21, 31.06it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "total size of new array must be unchanged",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[146], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m result \u001b[39m=\u001b[39m ts\u001b[39m.\u001b[39;49mtrainer\u001b[39m.\u001b[39;49moffpolicy_trainer(\n\u001b[0;32m      2\u001b[0m     policy, train_collector, test_collector,\n\u001b[0;32m      3\u001b[0m     max_epoch\u001b[39m=\u001b[39;49m\u001b[39m10\u001b[39;49m, step_per_epoch\u001b[39m=\u001b[39;49m\u001b[39m10000\u001b[39;49m, step_per_collect\u001b[39m=\u001b[39;49m\u001b[39m10\u001b[39;49m,\n\u001b[0;32m      4\u001b[0m     update_per_step\u001b[39m=\u001b[39;49m\u001b[39m0.1\u001b[39;49m, episode_per_test\u001b[39m=\u001b[39;49m\u001b[39m100\u001b[39;49m, batch_size\u001b[39m=\u001b[39;49m\u001b[39m64\u001b[39;49m,\n\u001b[0;32m      5\u001b[0m     train_fn\u001b[39m=\u001b[39;49m\u001b[39mlambda\u001b[39;49;00m epoch, env_step: policy\u001b[39m.\u001b[39;49mset_eps(\u001b[39m0.1\u001b[39;49m),\n\u001b[0;32m      6\u001b[0m     test_fn\u001b[39m=\u001b[39;49m\u001b[39mlambda\u001b[39;49;00m epoch, env_step: policy\u001b[39m.\u001b[39;49mset_eps(\u001b[39m0.05\u001b[39;49m),)\n\u001b[0;32m      7\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mFinished training! Use \u001b[39m\u001b[39m{\u001b[39;00mresult[\u001b[39m\"\u001b[39m\u001b[39mduration\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\SpiceBoi\\Desktop\\ML Projects\\mlenv\\Lib\\site-packages\\tianshou\\trainer\\offpolicy.py:133\u001b[0m, in \u001b[0;36moffpolicy_trainer\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    126\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39moffpolicy_trainer\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Dict[\u001b[39mstr\u001b[39m, Union[\u001b[39mfloat\u001b[39m, \u001b[39mstr\u001b[39m]]:  \u001b[39m# type: ignore\u001b[39;00m\n\u001b[0;32m    127\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Wrapper for OffPolicyTrainer run method.\u001b[39;00m\n\u001b[0;32m    128\u001b[0m \n\u001b[0;32m    129\u001b[0m \u001b[39m    It is identical to ``OffpolicyTrainer(...).run()``.\u001b[39;00m\n\u001b[0;32m    130\u001b[0m \n\u001b[0;32m    131\u001b[0m \u001b[39m    :return: See :func:`~tianshou.trainer.gather_info`.\u001b[39;00m\n\u001b[0;32m    132\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 133\u001b[0m     \u001b[39mreturn\u001b[39;00m OffpolicyTrainer(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\u001b[39m.\u001b[39;49mrun()\n",
      "File \u001b[1;32mc:\\Users\\SpiceBoi\\Desktop\\ML Projects\\mlenv\\Lib\\site-packages\\tianshou\\trainer\\base.py:441\u001b[0m, in \u001b[0;36mBaseTrainer.run\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    439\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    440\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mis_run \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m--> 441\u001b[0m     deque(\u001b[39mself\u001b[39;49m, maxlen\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m)  \u001b[39m# feed the entire iterator into a zero-length deque\u001b[39;00m\n\u001b[0;32m    442\u001b[0m     info \u001b[39m=\u001b[39m gather_info(\n\u001b[0;32m    443\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstart_time, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrain_collector, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtest_collector,\n\u001b[0;32m    444\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbest_reward, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbest_reward_std\n\u001b[0;32m    445\u001b[0m     )\n\u001b[0;32m    446\u001b[0m \u001b[39mfinally\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\SpiceBoi\\Desktop\\ML Projects\\mlenv\\Lib\\site-packages\\tianshou\\trainer\\base.py:299\u001b[0m, in \u001b[0;36mBaseTrainer.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    296\u001b[0m         result[\u001b[39m\"\u001b[39m\u001b[39mn/st\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mint\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgradient_step)\n\u001b[0;32m    297\u001b[0m         t\u001b[39m.\u001b[39mupdate()\n\u001b[1;32m--> 299\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpolicy_update_fn(data, result)\n\u001b[0;32m    300\u001b[0m     t\u001b[39m.\u001b[39mset_postfix(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mdata)\n\u001b[0;32m    302\u001b[0m \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mn \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m t\u001b[39m.\u001b[39mtotal \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstop_fn_flag:\n",
      "File \u001b[1;32mc:\\Users\\SpiceBoi\\Desktop\\ML Projects\\mlenv\\Lib\\site-packages\\tianshou\\trainer\\offpolicy.py:122\u001b[0m, in \u001b[0;36mOffpolicyTrainer.policy_update_fn\u001b[1;34m(self, data, result)\u001b[0m\n\u001b[0;32m    120\u001b[0m \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mround\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mupdate_per_step \u001b[39m*\u001b[39m result[\u001b[39m\"\u001b[39m\u001b[39mn/st\u001b[39m\u001b[39m\"\u001b[39m])):\n\u001b[0;32m    121\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgradient_step \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m--> 122\u001b[0m     losses \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpolicy\u001b[39m.\u001b[39;49mupdate(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbatch_size, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_collector\u001b[39m.\u001b[39;49mbuffer)\n\u001b[0;32m    123\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlog_update_data(data, losses)\n",
      "File \u001b[1;32mc:\\Users\\SpiceBoi\\Desktop\\ML Projects\\mlenv\\Lib\\site-packages\\tianshou\\policy\\base.py:276\u001b[0m, in \u001b[0;36mBasePolicy.update\u001b[1;34m(self, sample_size, buffer, **kwargs)\u001b[0m\n\u001b[0;32m    274\u001b[0m batch, indices \u001b[39m=\u001b[39m buffer\u001b[39m.\u001b[39msample(sample_size)\n\u001b[0;32m    275\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mupdating \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m--> 276\u001b[0m batch \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mprocess_fn(batch, buffer, indices)\n\u001b[0;32m    277\u001b[0m result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlearn(batch, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    278\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpost_process_fn(batch, buffer, indices)\n",
      "File \u001b[1;32mc:\\Users\\SpiceBoi\\Desktop\\ML Projects\\mlenv\\Lib\\site-packages\\tianshou\\policy\\modelfree\\dqn.py:106\u001b[0m, in \u001b[0;36mDQNPolicy.process_fn\u001b[1;34m(self, batch, buffer, indices)\u001b[0m\n\u001b[0;32m     98\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mprocess_fn\u001b[39m(\n\u001b[0;32m     99\u001b[0m     \u001b[39mself\u001b[39m, batch: Batch, buffer: ReplayBuffer, indices: np\u001b[39m.\u001b[39mndarray\n\u001b[0;32m    100\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Batch:\n\u001b[0;32m    101\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Compute the n-step return for Q-learning targets.\u001b[39;00m\n\u001b[0;32m    102\u001b[0m \n\u001b[0;32m    103\u001b[0m \u001b[39m    More details can be found at\u001b[39;00m\n\u001b[0;32m    104\u001b[0m \u001b[39m    :meth:`~tianshou.policy.BasePolicy.compute_nstep_return`.\u001b[39;00m\n\u001b[0;32m    105\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 106\u001b[0m     batch \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcompute_nstep_return(\n\u001b[0;32m    107\u001b[0m         batch, buffer, indices, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_target_q, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_gamma, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_n_step,\n\u001b[0;32m    108\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_rew_norm\n\u001b[0;32m    109\u001b[0m     )\n\u001b[0;32m    110\u001b[0m     \u001b[39mreturn\u001b[39;00m batch\n",
      "File \u001b[1;32mc:\\Users\\SpiceBoi\\Desktop\\ML Projects\\mlenv\\Lib\\site-packages\\tianshou\\policy\\base.py:396\u001b[0m, in \u001b[0;36mBasePolicy.compute_nstep_return\u001b[1;34m(batch, buffer, indice, target_q_fn, gamma, n_step, rew_norm)\u001b[0m\n\u001b[0;32m    394\u001b[0m end_flag \u001b[39m=\u001b[39m buffer\u001b[39m.\u001b[39mdone\u001b[39m.\u001b[39mcopy()\n\u001b[0;32m    395\u001b[0m end_flag[buffer\u001b[39m.\u001b[39munfinished_index()] \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m--> 396\u001b[0m target_q \u001b[39m=\u001b[39m _nstep_return(rew, end_flag, target_q, indices, gamma, n_step)\n\u001b[0;32m    398\u001b[0m batch\u001b[39m.\u001b[39mreturns \u001b[39m=\u001b[39m to_torch_as(target_q, target_q_torch)\n\u001b[0;32m    399\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(batch, \u001b[39m\"\u001b[39m\u001b[39mweight\u001b[39m\u001b[39m\"\u001b[39m):  \u001b[39m# prio buffer update\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\SpiceBoi\\Desktop\\ML Projects\\mlenv\\Lib\\site-packages\\numba\\np\\arrayobj.py:1987\u001b[0m, in \u001b[0;36mnormalize_reshape_value\u001b[1;34m()\u001b[0m\n\u001b[0;32m   1985\u001b[0m \u001b[39mif\u001b[39;00m num_neg_value \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m   1986\u001b[0m     \u001b[39mif\u001b[39;00m origsize \u001b[39m!=\u001b[39m known_size:\n\u001b[1;32m-> 1987\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mtotal size of new array must be unchanged\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m   1989\u001b[0m \u001b[39melif\u001b[39;00m num_neg_value \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m   1990\u001b[0m     \u001b[39m# Infer negative dimension\u001b[39;00m\n\u001b[0;32m   1991\u001b[0m     \u001b[39mif\u001b[39;00m known_size \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n",
      "\u001b[1;31mValueError\u001b[0m: total size of new array must be unchanged"
     ]
    }
   ],
   "source": [
    "result = ts.trainer.offpolicy_trainer(\n",
    "    policy, train_collector, test_collector,\n",
    "    max_epoch=10, step_per_epoch=10000, step_per_collect=10,\n",
    "    update_per_step=0.1, episode_per_test=100, batch_size=64,\n",
    "    train_fn=lambda epoch, env_step: policy.set_eps(0.1),\n",
    "    test_fn=lambda epoch, env_step: policy.set_eps(0.05),)\n",
    "print(f'Finished training! Use {result[\"duration\"]}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
